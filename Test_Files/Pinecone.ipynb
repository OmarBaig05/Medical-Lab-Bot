{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(\".env\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\D\\Work\\Projects\\Medical Lab Bot\\Test_Files\\.venv\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pinecone\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load your DataFrame\n",
    "df = pd.read_csv(\"../DATA/Embedded_Files/testing_tokens.csv\")  # Replace with your file path\n",
    "\n",
    "# Ensure the \"tokens\" column is in the correct format (list of floats)\n",
    "df[\"tokens\"] = df[\"tokens\"].apply(eval)  # Convert string representation of list to actual list\n",
    "\n",
    "# Chunk the data using LangChain\n",
    "chunk_size = 256\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "\n",
    "def chunk_embeddings(tokens, chunk_size):\n",
    "    \"\"\"Splits the embedding list into chunks of specified size.\"\"\"\n",
    "    return [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "# Create or connect to a Pinecone index\n",
    "index_name = \"medical-tests-index\"\n",
    "\n",
    "\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Function to extract text from PDF and save to txt file\n",
    "def save_pdf_text(pdf_path, output_folder):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "    file_number = len(os.listdir(output_folder)) + 1\n",
    "    output_path = os.path.join(output_folder, f\"{file_number}.txt\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    return output_path\n",
    "\n",
    "# Function to extract headings from text\n",
    "def extract_headings_and_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    extracted_data = []\n",
    "    current_heading = \"Unknown\"\n",
    "    buffer = []\n",
    "    for line in lines:\n",
    "        if line.isupper() and len(line) > 3:  # Heuristic for headings\n",
    "            if buffer:\n",
    "                extracted_data.append((current_heading, \" \".join(buffer)))\n",
    "                buffer = []\n",
    "            current_heading = line.strip()\n",
    "        else:\n",
    "            buffer.append(line.strip())\n",
    "    if buffer:\n",
    "        extracted_data.append((current_heading, \" \".join(buffer)))\n",
    "    return extracted_data\n",
    "\n",
    "# Function to create embeddings\n",
    "def embeddingCreator(text, modelName=\"o200k_base\"):\n",
    "    encoding = tiktoken.get_encoding(modelName)\n",
    "    return encoding.encode(text)\n",
    "\n",
    "# Function to split embeddings into chunks\n",
    "def chunk_embeddings(tokens, chunk_size):\n",
    "    return [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: 159358_AMAGlossaryofMedicalTerms_Ver1.0.pdf\n",
      "Processing PDF: FDMT503ABloodChemHndts.pdf\n",
      "Processing PDF: interpretation-of-full-blood-count-parameters-in-health-and-disease.pdf\n",
      "Processing PDF: Interpreting+Laboratory+Tests.pdf\n",
      "Processing PDF: laboratory-reference-ranges.pdf\n",
      "Processing PDF: Lab_Values_Table_PSAP.pdf\n",
      "Processing PDF: ln_hematology_mlt_final.pdf\n",
      "Processing PDF: main.pdf\n",
      "Processing PDF: Oxford-Handbook-of-Clinical-and-Laboratory-Investigation.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process PDF files and save text\n",
    "pdf_folder = \"../DATA/Books/New folder\"\n",
    "txt_folder = \"../DATA/TextFiles\"\n",
    "os.makedirs(txt_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "for file in os.listdir(pdf_folder):\n",
    "    file_path = os.path.join(pdf_folder, file)\n",
    "    if os.path.isfile(file_path) and file.endswith(\".pdf\"):\n",
    "        print(f\"Processing PDF: {file}\")\n",
    "        save_pdf_text(file_path, txt_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process saved text files\n",
    "txt_folder = \"../DATA/TextFiles\"\n",
    "data_to_upsert = []\n",
    "txt_files = sorted(os.listdir(txt_folder), key=lambda x: int(x.split(\".\")[0]))\n",
    "for txt_file in txt_files:\n",
    "    txt_path = os.path.join(txt_folder, txt_file)\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text_data = f.read()\n",
    "    \n",
    "    extracted_data = extract_headings_and_text(text_data)\n",
    "    text_data = \" \".join([text for _, text in extracted_data])\n",
    "    tokens = embeddingCreator(text_data)\n",
    "    token_chunks = chunk_embeddings(tokens, 256)\n",
    "\n",
    "    heading_index = 0\n",
    "    for chunk_idx, chunk in enumerate(token_chunks):\n",
    "        if len(chunk) == 256:\n",
    "            while heading_index < len(extracted_data) - 1 and len(extracted_data[heading_index][1]) < 256:\n",
    "                heading_index += 1\n",
    "            assigned_heading = extracted_data[heading_index][0]\n",
    "            \n",
    "            vector_id = f\"{txt_file}_chunk_{chunk_idx}\"\n",
    "            metadata = {\"test_name\": assigned_heading, \"source\": txt_file, \"url\": \"NaN\"}\n",
    "            data_to_upsert.append({\"id\": vector_id, \"values\": chunk, \"metadata\": metadata})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded batch 1 of 24\n",
      "Uploaded batch 2 of 24\n",
      "Uploaded batch 3 of 24\n",
      "Uploaded batch 4 of 24\n",
      "Uploaded batch 5 of 24\n",
      "Uploaded batch 6 of 24\n",
      "Uploaded batch 7 of 24\n",
      "Uploaded batch 8 of 24\n",
      "Uploaded batch 9 of 24\n",
      "Uploaded batch 10 of 24\n",
      "Uploaded batch 11 of 24\n",
      "Uploaded batch 12 of 24\n",
      "Uploaded batch 13 of 24\n",
      "Uploaded batch 14 of 24\n",
      "Uploaded batch 15 of 24\n",
      "Uploaded batch 16 of 24\n",
      "Uploaded batch 17 of 24\n",
      "Uploaded batch 18 of 24\n",
      "Uploaded batch 19 of 24\n",
      "Uploaded batch 20 of 24\n",
      "Uploaded batch 21 of 24\n",
      "Uploaded batch 22 of 24\n",
      "Uploaded batch 23 of 24\n",
      "Uploaded batch 24 of 24\n",
      "All data uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Batch upload to Pinecone\n",
    "batch_size = 100\n",
    "for i in range(0, len(data_to_upsert), batch_size):\n",
    "    batch = data_to_upsert[i:i + batch_size]\n",
    "    # Convert all values in the chunk to float\n",
    "    for item in batch:\n",
    "        item['values'] = [float(x) for x in item['values']]\n",
    "    index.upsert(vectors=batch, namespace=\"ns1\")\n",
    "    print(f\"Uploaded batch {i // batch_size + 1} of {len(data_to_upsert) // batch_size + 1}\")\n",
    "\n",
    "print(\"All data uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pinecone\n",
    "\n",
    "def upsert_data_to_pinecone(df, chunk_size=256, batch_size=100):\n",
    "\n",
    "    # Ensure the \"tokens\" column is in the correct format (list of floats)\n",
    "    df[\"tokens\"] = df[\"tokens\"].apply(eval)  # Convert string representation of list to actual list\n",
    "\n",
    "    # Chunk the data using LangChain\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "\n",
    "    def chunk_embeddings(tokens, chunk_size):\n",
    "        \"\"\"Splits the embedding list into chunks of specified size.\"\"\"\n",
    "        return [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "    # Create or connect to a Pinecone index\n",
    "    index_name = \"medical-tests-index\"\n",
    "\n",
    "    index = pc.Index(index_name)\n",
    "\n",
    "    # Prepare data for upserting into Pinecone\n",
    "    print(\"Uploading data to Pinecone...\")\n",
    "    data_to_upsert = []\n",
    "    for idx, row in df.iterrows():\n",
    "        token_chunks = chunk_embeddings(row[\"tokens\"], chunk_size)\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(token_chunks):\n",
    "            if len(chunk) == chunk_size:  # Ensure valid chunk size\n",
    "                vector_id = f\"vec_{idx}_chunk_{chunk_idx}\"\n",
    "                metadata = {\n",
    "                    \"test_name\": row[\"Test Name\"],\n",
    "                    \"source\": row[\"Source\"],\n",
    "                    \"url\": row[\"URL\"]\n",
    "                }\n",
    "                # Convert all values in the chunk to float\n",
    "                chunk = [float(x) for x in chunk]\n",
    "                data_to_upsert.append({\"id\": vector_id, \"values\": chunk, \"metadata\": metadata})\n",
    "\n",
    "    # Batch upload\n",
    "    for i in range(0, len(data_to_upsert), batch_size):\n",
    "        batch = data_to_upsert[i:i + batch_size]\n",
    "        index.upsert(vectors=batch)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Uploaded batch {i // batch_size + 1} of {len(data_to_upsert) // batch_size + 1}\")\n",
    "\n",
    "    index.upsert(vectors=batch)\n",
    "    print(\"Data upload complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"../DATA/Embedded_Files/medlinePlus_tokens.csv\")  # Replace with your file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to Pinecone...\n",
      "Uploaded batch 1 of 15\n",
      "Uploaded batch 2 of 15\n",
      "Uploaded batch 3 of 15\n",
      "Uploaded batch 4 of 15\n",
      "Uploaded batch 5 of 15\n",
      "Uploaded batch 6 of 15\n",
      "Uploaded batch 7 of 15\n",
      "Uploaded batch 8 of 15\n",
      "Uploaded batch 9 of 15\n",
      "Uploaded batch 10 of 15\n",
      "Uploaded batch 11 of 15\n",
      "Uploaded batch 12 of 15\n",
      "Uploaded batch 13 of 15\n",
      "Uploaded batch 14 of 15\n",
      "Uploaded batch 15 of 15\n",
      "Data upload complete!\n"
     ]
    }
   ],
   "source": [
    "upsert_data_to_pinecone(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks with 'Urine Protein And Urine Protein Creatinine Ratio': 0\n"
     ]
    }
   ],
   "source": [
    "# Query Pinecone to count matching chunks\n",
    "query_filter = {\"test_name\": {\"$eq\": \"HIV Viral Load\"}}\n",
    "\n",
    "# Fetch metadata-matching chunks\n",
    "fetch_result = index.query(vector=[0] * chunk_size,  # Dummy vector\n",
    "                           top_k=10000,  # Fetch a large number to ensure all results are included\n",
    "                           filter=query_filter,\n",
    "                           namespace=\"ns1\",\n",
    "                           include_metadata=True)\n",
    "\n",
    "# Count matching chunks\n",
    "matching_chunks_count = len(fetch_result[\"matches\"])\n",
    "print(f\"Number {matching_chunks_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
