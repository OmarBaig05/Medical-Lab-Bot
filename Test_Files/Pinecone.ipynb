{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(\".env\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\D\\Work\\Projects\\Medical Lab Bot\\Test_Files\\.venv\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pinecone\n",
    "\n",
    "# Load your DataFrame\n",
    "df = pd.read_csv(\"../DATA/Embedded_Files/testing_tokens.csv\")  # Replace with your file path\n",
    "\n",
    "# Ensure the \"Embedding\" column is in the correct format (list of floats)\n",
    "df[\"tokens\"] = df[\"tokens\"].apply(eval)  # Convert string representation of list to actual list\n",
    "\n",
    "# Check the dimension of the vectors\n",
    "vector_dimension = len(df[\"tokens\"].iloc[0])\n",
    "print(f\"Vector dimension: {vector_dimension}\")\n",
    "\n",
    "# Create or connect to a Pinecone index\n",
    "index_name = \"medical-tests-index\"\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=vector_dimension, # Use the correct dimension\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Prepare data for upsert\n",
    "vectors_to_upsert = []\n",
    "for idx, row in df.iterrows():\n",
    "    # Extract embedding and metadata\n",
    "    embedding = [float(x) for x in row[\"tokens\"]]\n",
    "    metadata = {\n",
    "        \"test_name\": row[\"Test Name\"],\n",
    "        \"source\": row[\"Source\"],\n",
    "        \"url\": row[\"URL\"]\n",
    "    }\n",
    "    \n",
    "    # Create a unique ID for each vector (e.g., using the row index)\n",
    "    vector_id = f\"vec_{idx}\"\n",
    "    \n",
    "    # Append to the list of vectors to upsert\n",
    "    vectors_to_upsert.append((vector_id, embedding, metadata))\n",
    "\n",
    "# Upsert data into Pinecone\n",
    "index.upsert(vectors=vectors_to_upsert)\n",
    "\n",
    "print(f\"Upserted {len(vectors_to_upsert)} vectors into Pinecone index '{index_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.16-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached SQLAlchemy-2.0.37-cp311-cp311-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached aiohttp-3.11.11-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.32 (from langchain)\n",
      "  Downloading langchain_core-0.3.32-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.2-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy<2,>=1.22.4 (from langchain)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in e:\\d\\work\\projects\\medical lab bot\\test_files\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached propcache-0.2.1-cp311-cp311-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.18.3-cp311-cp311-win_amd64.whl.metadata (71 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.32->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in e:\\d\\work\\projects\\medical lab bot\\test_files\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.32->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in e:\\d\\work\\projects\\medical lab bot\\test_files\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.32->langchain) (4.12.2)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.15-cp311-cp311-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached pydantic_core-2.27.2-cp311-cp311-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\d\\work\\projects\\medical lab bot\\test_files\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\d\\work\\projects\\medical lab bot\\test_files\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\d\\work\\projects\\medical lab bot\\test_files\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\d\\work\\projects\\medical lab bot\\test_files\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.1.1-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading langchain-0.3.16-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.5/1.0 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.5/1.0 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 0.8/1.0 MB 763.2 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 0.8/1.0 MB 763.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 736.8 kB/s eta 0:00:00\n",
      "Using cached aiohttp-3.11.11-cp311-cp311-win_amd64.whl (442 kB)\n",
      "Downloading langchain_core-0.3.32-py3-none-any.whl (412 kB)\n",
      "Downloading langchain_text_splitters-0.3.5-py3-none-any.whl (31 kB)\n",
      "Downloading langsmith-0.3.2-py3-none-any.whl (333 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Using cached SQLAlchemy-2.0.37-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
      "Using cached greenlet-3.1.1-cp311-cp311-win_amd64.whl (298 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Downloading orjson-3.10.15-cp311-cp311-win_amd64.whl (133 kB)\n",
      "Using cached propcache-0.2.1-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached yarl-1.18.3-cp311-cp311-win_amd64.whl (91 kB)\n",
      "Downloading zstandard-0.23.0-cp311-cp311-win_amd64.whl (495 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zstandard, tenacity, sniffio, PyYAML, pydantic-core, propcache, orjson, numpy, multidict, jsonpointer, h11, greenlet, frozenlist, attrs, annotated-types, aiohappyeyeballs, yarl, SQLAlchemy, requests-toolbelt, pydantic, jsonpatch, httpcore, anyio, aiosignal, httpx, aiohttp, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.2\n",
      "    Uninstalling numpy-2.2.2:\n",
      "      Successfully uninstalled numpy-2.2.2\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.37 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.8.0 attrs-25.1.0 frozenlist-1.5.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.16 langchain-core-0.3.32 langchain-text-splitters-0.3.5 langsmith-0.3.2 multidict-6.1.0 numpy-1.26.4 orjson-3.10.15 propcache-0.2.1 pydantic-2.10.6 pydantic-core-2.27.2 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.0.0 yarl-1.18.3 zstandard-0.23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'E:\\D\\Work\\Projects\\Medical Lab Bot\\Test_Files\\.venv\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'E:\\D\\Work\\Projects\\Medical Lab Bot\\Test_Files\\.venv\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pinecone\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load your DataFrame\n",
    "df = pd.read_csv(\"../DATA/Embedded_Files/testing_tokens.csv\")  # Replace with your file path\n",
    "\n",
    "# Ensure the \"tokens\" column is in the correct format (list of floats)\n",
    "df[\"tokens\"] = df[\"tokens\"].apply(eval)  # Convert string representation of list to actual list\n",
    "\n",
    "# Chunk the data using LangChain\n",
    "chunk_size = 256\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "\n",
    "def chunk_embeddings(tokens, chunk_size):\n",
    "    \"\"\"Splits the embedding list into chunks of specified size.\"\"\"\n",
    "    return [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "# Create or connect to a Pinecone index\n",
    "index_name = \"medical-tests-index\"\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=chunk_size, # Use the correct dimension\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")\n",
    "\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pinecone\n",
    "\n",
    "def upsert_data_to_pinecone(df, chunk_size=256, batch_size=100):\n",
    "\n",
    "    # Ensure the \"tokens\" column is in the correct format (list of floats)\n",
    "    df[\"tokens\"] = df[\"tokens\"].apply(eval)  # Convert string representation of list to actual list\n",
    "\n",
    "    # Chunk the data using LangChain\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "\n",
    "    def chunk_embeddings(tokens, chunk_size):\n",
    "        \"\"\"Splits the embedding list into chunks of specified size.\"\"\"\n",
    "        return [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "    # Create or connect to a Pinecone index\n",
    "    index_name = \"medical-tests-index\"\n",
    "\n",
    "    index = pc.Index(index_name)\n",
    "\n",
    "    # Prepare data for upserting into Pinecone\n",
    "    print(\"Uploading data to Pinecone...\")\n",
    "    data_to_upsert = []\n",
    "    for idx, row in df.iterrows():\n",
    "        token_chunks = chunk_embeddings(row[\"tokens\"], chunk_size)\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(token_chunks):\n",
    "            if len(chunk) == chunk_size:  # Ensure valid chunk size\n",
    "                vector_id = f\"vec_{idx}_chunk_{chunk_idx}\"\n",
    "                metadata = {\n",
    "                    \"test_name\": row[\"Test Name\"],\n",
    "                    \"source\": row[\"Source\"],\n",
    "                    \"url\": row[\"URL\"]\n",
    "                }\n",
    "                # Convert all values in the chunk to float\n",
    "                chunk = [float(x) for x in chunk]\n",
    "                data_to_upsert.append({\"id\": vector_id, \"values\": chunk, \"metadata\": metadata})\n",
    "\n",
    "    # Batch upload\n",
    "    for i in range(0, len(data_to_upsert), batch_size):\n",
    "        batch = data_to_upsert[i:i + batch_size]\n",
    "        index.upsert(vectors=batch)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Uploaded batch {i // batch_size + 1} of {len(data_to_upsert) // batch_size + 1}\")\n",
    "\n",
    "    index.upsert(vectors=batch)\n",
    "    print(\"Data upload complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"../DATA/Embedded_Files/medlinePlus_tokens.csv\")  # Replace with your file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to Pinecone...\n",
      "Uploaded batch 1 of 15\n",
      "Uploaded batch 2 of 15\n",
      "Uploaded batch 3 of 15\n",
      "Uploaded batch 4 of 15\n",
      "Uploaded batch 5 of 15\n",
      "Uploaded batch 6 of 15\n",
      "Uploaded batch 7 of 15\n",
      "Uploaded batch 8 of 15\n",
      "Uploaded batch 9 of 15\n",
      "Uploaded batch 10 of 15\n",
      "Uploaded batch 11 of 15\n",
      "Uploaded batch 12 of 15\n",
      "Uploaded batch 13 of 15\n",
      "Uploaded batch 14 of 15\n",
      "Uploaded batch 15 of 15\n",
      "Data upload complete!\n"
     ]
    }
   ],
   "source": [
    "upsert_data_to_pinecone(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks with 'Urine Protein And Urine Protein Creatinine Ratio': 0\n"
     ]
    }
   ],
   "source": [
    "# Query Pinecone to count matching chunks\n",
    "query_filter = {\"test_name\": {\"$eq\": \"HIV Viral Load\"}}\n",
    "\n",
    "# Fetch metadata-matching chunks\n",
    "fetch_result = index.query(vector=[0] * chunk_size,  # Dummy vector\n",
    "                           top_k=10000,  # Fetch a large number to ensure all results are included\n",
    "                           filter=query_filter,\n",
    "                           namespace=\"ns1\",\n",
    "                           include_metadata=True)\n",
    "\n",
    "# Count matching chunks\n",
    "matching_chunks_count = len(fetch_result[\"matches\"])\n",
    "print(f\"Number {matching_chunks_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
