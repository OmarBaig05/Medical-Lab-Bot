{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../DATA/Scraped_Data/medical_tests_interpretation.csv\")\n",
    "df2 = pd.read_csv(\"../DATA/Scraped_Data/testing_scraped_content.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def process_csv(df, source):\n",
    "    \n",
    "    # Initialize an empty list to store the new rows\n",
    "    df['Description'] = df['Description'].fillna('')\n",
    "    new_rows = []\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        test_name = row['Test Name']\n",
    "        description = row['Description']\n",
    "        url = row['URL']\n",
    "        \n",
    "        # Chunk the description into 128-word chunks\n",
    "        chunks = chunk_text(description, chunk_size=128)\n",
    "        \n",
    "        # Create a new row for each chunk\n",
    "        for chunk in chunks:\n",
    "            new_row = {\n",
    "                'Test Name': test_name,\n",
    "                'Description': chunk,\n",
    "                'Source': source,\n",
    "                'URL': url\n",
    "            }\n",
    "            new_rows.append(new_row)\n",
    "    \n",
    "    # Create a new DataFrame from the list of new rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_df = process_csv(df,source=  \"medlineplus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the new DataFrame to a CSV file\n",
    "med_df.to_csv(f\"medlineplus_chunks_{CHUNK_SIZE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = process_csv(df2, source=\"testing.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(f\"testing_chunks_{CHUNK_SIZE}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT DATA FROM PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    return \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "\n",
    "# Function to split text into chunks\n",
    "def create_text_chunks(text, chunk_size=CHUNK_SIZE):\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "# Function to extract headings and map content to them\n",
    "def extract_headings_and_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    extracted_data = []\n",
    "    current_heading = None\n",
    "    buffer = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.isupper() and len(line) > 3:  # Heuristic for headings\n",
    "            if buffer:\n",
    "                extracted_data.append((current_heading, \" \".join(buffer)))\n",
    "                buffer = []\n",
    "            current_heading = line.strip()\n",
    "        else:\n",
    "            buffer.append(line.strip())\n",
    "    \n",
    "    if buffer:\n",
    "        extracted_data.append((current_heading, \" \".join(buffer)))\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "# Function to create a structured DataFrame\n",
    "def create_dataframe_from_text_files(txt_folder):\n",
    "    data_list = []\n",
    "    txt_files = sorted(os.listdir(txt_folder), key=lambda x: int(x.split(\".\")[0]))\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        txt_path = os.path.join(txt_folder, txt_file)\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text_data = f.read()\n",
    "        \n",
    "        extracted_data = extract_headings_and_text(text_data)\n",
    "        text_chunks = create_text_chunks(text_data)\n",
    "        \n",
    "        for chunk in text_chunks:\n",
    "            test_name, source = \"No_testName\", \"No_Source\"\n",
    "            \n",
    "            for heading, content in extracted_data:\n",
    "                if chunk in content:\n",
    "                    test_name = heading\n",
    "                    source = heading\n",
    "                    break\n",
    "            \n",
    "            data_list.append({\n",
    "                \"Test Name\": test_name,\n",
    "                \"Description\": chunk,\n",
    "                \"Source\": source,\n",
    "                \"URL\": \"No_URl\"\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data_list)\n",
    "\n",
    "# Function to save DataFrame to a CSV file\n",
    "def save_dataframe(df, output_path):\n",
    "    df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing complete. CSV file saved.\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "txt_folder = \"../DATA/TextFiles\"\n",
    "df = create_dataframe_from_text_files(txt_folder)\n",
    "save_dataframe(df, f\"books_{CHUNK_SIZE}.csv\")\n",
    "\n",
    "print(\"Data processing complete. CSV file saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\D\\Work\\Projects\\Medical Lab Bot\\Test_Files\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\D\\Work\\Projects\\Medical Lab Bot\\Test_Files\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\omarb\\.cache\\huggingface\\hub\\models--sentence-transformers--msmarco-bert-base-dot-v5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer('sentence-transformers/msmarco-bert-base-dot-v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load CSV files\n",
    "df1 = pd.read_csv(f\"../DATA/Chunked_data/medlineplus_chunks_{CHUNK_SIZE}.csv\")\n",
    "df2 = pd.read_csv(f\"../DATA/Chunked_data/testing_chunks_{CHUNK_SIZE}.csv\")\n",
    "df3 = pd.read_csv(f\"../DATA/Chunked_data/books_{CHUNK_SIZE}.csv\")\n",
    "\n",
    "# Load the embedding model once (reduces overhead)\n",
    "\n",
    "\n",
    "def embeddingCreator(text):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given text using SentenceTransformer.\n",
    "    \"\"\"\n",
    "    return embedding_model.encode(text).tolist()\n",
    "\n",
    "def Scrapper_With_Source(df, source):\n",
    "    \"\"\"\n",
    "    Extracts embeddings and metadata from a DataFrame when a fixed source is given.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for i, row in df.iterrows():\n",
    "        text = row[\"Description\"]\n",
    "        test_name = row[\"Test Name\"]\n",
    "        url = row[\"URL\"]\n",
    "\n",
    "        tokens = embeddingCreator(text)\n",
    "        data.append({\"Test Name\": test_name, \"tokens\": tokens, \"text\": text, \"Source\": source, \"URL\": url})\n",
    "        \n",
    "        if i % 100 == 0:  # Print progress every 100 rows\n",
    "            print(f\"Processed {i}/{len(df)} rows (Source: {source})\")\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def Scrapper_With_Out_Source(df):\n",
    "    \"\"\"\n",
    "    Extracts embeddings and metadata from a DataFrame when the source column exists.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for i, row in df.iterrows():\n",
    "        text = row[\"Description\"]\n",
    "        test_name = row[\"Test Name\"]\n",
    "        url = row[\"URL\"]\n",
    "        source = row[\"Source\"]\n",
    "\n",
    "        tokens = embeddingCreator(text)\n",
    "        data.append({\"Test Name\": test_name, \"tokens\": tokens, \"text\": text, \"Source\": source, \"URL\": url})\n",
    "\n",
    "        if i % 100 == 0:  # Print progress every 100 rows\n",
    "            print(f\"Processed {i}/{len(df)} rows (Dynamic Source)\")\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def writeToFile(df_tokens, filename):\n",
    "    \"\"\"\n",
    "    Writes the DataFrame to a CSV file.\n",
    "    \"\"\"\n",
    "    df_tokens.to_csv(filename, index=False, encoding=\"utf-8\")\n",
    "\n",
    "def Create_Embedding_With_Source(df, source, path):\n",
    "    \"\"\"\n",
    "    Wrapper function to process a DataFrame and store embeddings when source is known.\n",
    "    \"\"\"\n",
    "    df[\"Description\"] = df[\"Description\"].fillna('')\n",
    "    df_tokens = Scrapper_With_Source(df, source)\n",
    "    writeToFile(df_tokens, path)\n",
    "\n",
    "def Create_Embedding_With_Out_Source(df, path):\n",
    "    \"\"\"\n",
    "    Wrapper function to process a DataFrame and store embeddings when source is in CSV.\n",
    "    \"\"\"\n",
    "    df[\"Description\"] = df[\"Description\"].fillna('')\n",
    "    df_tokens = Scrapper_With_Out_Source(df)\n",
    "    writeToFile(df_tokens, path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/3449 rows (Source: testing.com)\n",
      "Processed 0/2803 rows (Source: medlinePlus.com)\n",
      "Processed 0/1148 rows (Dynamic Source)\n",
      "Processed 100/2803 rows (Source: medlinePlus.com)\n",
      "Processed 100/3449 rows (Source: testing.com)\n",
      "Processed 200/2803 rows (Source: medlinePlus.com)\n",
      "Processed 100/1148 rows (Dynamic Source)\n",
      "Processed 200/3449 rows (Source: testing.com)\n",
      "Processed 300/2803 rows (Source: medlinePlus.com)\n",
      "Processed 300/3449 rows (Source: testing.com)\n",
      "Processed 400/2803 rows (Source: medlinePlus.com)\n",
      "Processed 200/1148 rows (Dynamic Source)\n",
      "Processed 400/3449 rows (Source: testing.com)\n",
      "Processed 500/2803 rows (Source: medlinePlus.com)\n",
      "Processed 500/3449 rows (Source: testing.com)\n",
      "Processed 600/2803 rows (Source: medlinePlus.com)\n",
      "Processed 300/1148 rows (Dynamic Source)\n",
      "Processed 600/3449 rows (Source: testing.com)\n",
      "Processed 700/2803 rows (Source: medlinePlus.com)\n",
      "Processed 700/3449 rows (Source: testing.com)\n",
      "Processed 800/2803 rows (Source: medlinePlus.com)\n",
      "Processed 400/1148 rows (Dynamic Source)\n",
      "Processed 800/3449 rows (Source: testing.com)\n",
      "Processed 900/2803 rows (Source: medlinePlus.com)\n",
      "Processed 900/3449 rows (Source: testing.com)\n",
      "Processed 1000/2803 rows (Source: medlinePlus.com)\n",
      "Processed 500/1148 rows (Dynamic Source)\n",
      "Processed 1000/3449 rows (Source: testing.com)\n",
      "Processed 1100/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1200/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1100/3449 rows (Source: testing.com)\n",
      "Processed 1300/2803 rows (Source: medlinePlus.com)\n",
      "Processed 600/1148 rows (Dynamic Source)\n",
      "Processed 1200/3449 rows (Source: testing.com)\n",
      "Processed 1400/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1300/3449 rows (Source: testing.com)\n",
      "Processed 1500/2803 rows (Source: medlinePlus.com)\n",
      "Processed 700/1148 rows (Dynamic Source)\n",
      "Processed 1400/3449 rows (Source: testing.com)\n",
      "Processed 1600/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1500/3449 rows (Source: testing.com)\n",
      "Processed 1700/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1600/3449 rows (Source: testing.com)\n",
      "Processed 800/1148 rows (Dynamic Source)\n",
      "Processed 1800/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1700/3449 rows (Source: testing.com)\n",
      "Processed 1900/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1800/3449 rows (Source: testing.com)\n",
      "Processed 900/1148 rows (Dynamic Source)\n",
      "Processed 2000/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1900/3449 rows (Source: testing.com)\n",
      "Processed 2100/2803 rows (Source: medlinePlus.com)\n",
      "Processed 2000/3449 rows (Source: testing.com)\n",
      "Processed 2200/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1000/1148 rows (Dynamic Source)\n",
      "Processed 2100/3449 rows (Source: testing.com)\n",
      "Processed 2300/2803 rows (Source: medlinePlus.com)\n",
      "Processed 2400/2803 rows (Source: medlinePlus.com)\n",
      "Processed 2200/3449 rows (Source: testing.com)\n",
      "Processed 1100/1148 rows (Dynamic Source)\n",
      "Processed 2500/2803 rows (Source: medlinePlus.com)\n",
      "Processed 2300/3449 rows (Source: testing.com)\n",
      "✅ Successfully processed files\n",
      "Processed 2600/2803 rows (Source: medlinePlus.com)\n",
      "Processed 2400/3449 rows (Source: testing.com)\n",
      "Processed 2700/2803 rows (Source: medlinePlus.com)\n",
      "Processed 2500/3449 rows (Source: testing.com)\n",
      "Processed 2800/2803 rows (Source: medlinePlus.com)\n",
      "✅ Successfully processed medlineplus\n",
      "Processed 2600/3449 rows (Source: testing.com)\n",
      "Processed 2700/3449 rows (Source: testing.com)\n",
      "Processed 2800/3449 rows (Source: testing.com)\n",
      "Processed 2900/3449 rows (Source: testing.com)\n",
      "Processed 3000/3449 rows (Source: testing.com)\n",
      "Processed 3100/3449 rows (Source: testing.com)\n",
      "Processed 3200/3449 rows (Source: testing.com)\n",
      "Processed 3300/3449 rows (Source: testing.com)\n",
      "Processed 3400/3449 rows (Source: testing.com)\n",
      "✅ Successfully processed testing\n"
     ]
    }
   ],
   "source": [
    "# Parallel execution of embedding creation\n",
    "if __name__ == \"__main__\":\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = {\n",
    "            executor.submit(Create_Embedding_With_Out_Source, df3, f\"../DATA/Embedded_Files/files_tokens_{CHUNK_SIZE}.csv\"): \"files\",\n",
    "            executor.submit(Create_Embedding_With_Source, df1, f\"medlinePlus.com\", \"../DATA/Embedded_Files/medlinePlus_tokens_{CHUNK_SIZE}.csv\"): \"medlineplus\",\n",
    "            executor.submit(Create_Embedding_With_Source, df2, f\"testing.com\", \"../DATA/Embedded_Files/testing_com_tokens_{CHUNK_SIZE}.csv\"): \"testing\",\n",
    "        }\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            task_name = futures[future]\n",
    "            try:\n",
    "                future.result()  # This will raise exceptions if any occur\n",
    "                print(f\"✅ Successfully processed {task_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing {task_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
