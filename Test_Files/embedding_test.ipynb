{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\D\\Work\\Projects\\Medical Lab Bot\\Test_Files\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../DATA/Scraped_Data/medical_tests_interpretation.csv\")\n",
    "df2 = pd.read_csv(\"../DATA/Scraped_Data/testing_scraped_content.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def chunk_text(text, chunk_size=128):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def process_csv(df, output_csv, source):\n",
    "    \n",
    "    # Initialize an empty list to store the new rows\n",
    "    df['Description'] = df['Description'].fillna('')\n",
    "    new_rows = []\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        test_name = row['Test Name']\n",
    "        description = row['Description']\n",
    "        url = row['URL']\n",
    "        \n",
    "        # Chunk the description into 128-word chunks\n",
    "        chunks = chunk_text(description, chunk_size=128)\n",
    "        \n",
    "        # Create a new row for each chunk\n",
    "        for chunk in chunks:\n",
    "            new_row = {\n",
    "                'Test Name': test_name,\n",
    "                'Description': chunk,\n",
    "                'Source': source,\n",
    "                'URL': url\n",
    "            }\n",
    "            new_rows.append(new_row)\n",
    "    \n",
    "    # Create a new DataFrame from the list of new rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    \n",
    "    # Write the new DataFrame to a CSV file\n",
    "    new_df.to_csv(output_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_csv(df, \"medlineplus_chunks.csv\",source=  \"medlineplus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_csv(df2, \"testing_chunks.csv\", source=\"testing.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0/3449 rows (Source: testing.com)\n",
      "Processed 0/2803 rows (Source: medlinePlus.com)\n",
      "Processed 0/2906 rows (Dynamic Source)\n",
      "Processed 100/3449 rows (Source: testing.com)\n",
      "Processed 100/2803 rows (Source: medlinePlus.com)\n",
      "Processed 100/2906 rows (Dynamic Source)\n",
      "Processed 200/3449 rows (Source: testing.com)\n",
      "Processed 200/2803 rows (Source: medlinePlus.com)\n",
      "Processed 200/2906 rows (Dynamic Source)\n",
      "Processed 300/3449 rows (Source: testing.com)\n",
      "Processed 300/2803 rows (Source: medlinePlus.com)\n",
      "Processed 300/2906 rows (Dynamic Source)\n",
      "Processed 400/3449 rows (Source: testing.com)\n",
      "Processed 400/2803 rows (Source: medlinePlus.com)\n",
      "Processed 400/2906 rows (Dynamic Source)\n",
      "Processed 500/3449 rows (Source: testing.com)\n",
      "Processed 500/2803 rows (Source: medlinePlus.com)\n",
      "Processed 500/2906 rows (Dynamic Source)\n",
      "Processed 600/3449 rows (Source: testing.com)\n",
      "Processed 600/2803 rows (Source: medlinePlus.com)\n",
      "Processed 600/2906 rows (Dynamic Source)\n",
      "Processed 700/3449 rows (Source: testing.com)\n",
      "Processed 700/2803 rows (Source: medlinePlus.com)\n",
      "Processed 800/3449 rows (Source: testing.com)\n",
      "Processed 700/2906 rows (Dynamic Source)\n",
      "Processed 800/2803 rows (Source: medlinePlus.com)\n",
      "Processed 900/3449 rows (Source: testing.com)\n",
      "Processed 900/2803 rows (Source: medlinePlus.com)\n",
      "Processed 800/2906 rows (Dynamic Source)\n",
      "Processed 1000/3449 rows (Source: testing.com)\n",
      "Processed 1000/2803 rows (Source: medlinePlus.com)\n",
      "Processed 900/2906 rows (Dynamic Source)\n",
      "Processed 1100/3449 rows (Source: testing.com)\n",
      "Processed 1100/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1000/2906 rows (Dynamic Source)\n",
      "Processed 1200/3449 rows (Source: testing.com)\n",
      "Processed 1200/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1100/2906 rows (Dynamic Source)\n",
      "Processed 1300/3449 rows (Source: testing.com)\n",
      "Processed 1300/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1200/2906 rows (Dynamic Source)\n",
      "Processed 1400/3449 rows (Source: testing.com)\n",
      "Processed 1400/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1500/3449 rows (Source: testing.com)\n",
      "Processed 1300/2906 rows (Dynamic Source)\n",
      "Processed 1500/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1600/3449 rows (Source: testing.com)\n",
      "Processed 1400/2906 rows (Dynamic Source)\n",
      "Processed 1600/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1700/3449 rows (Source: testing.com)\n",
      "Processed 1700/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1500/2906 rows (Dynamic Source)\n",
      "Processed 1800/3449 rows (Source: testing.com)\n",
      "Processed 1800/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1600/2906 rows (Dynamic Source)\n",
      "Processed 1900/3449 rows (Source: testing.com)\n",
      "Processed 1900/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1700/2906 rows (Dynamic Source)\n",
      "Processed 2000/3449 rows (Source: testing.com)\n",
      "Processed 2000/2803 rows (Source: medlinePlus.com)\n",
      "Processed 1800/2906 rows (Dynamic Source)\n",
      "Processed 2100/3449 rows (Source: testing.com)\n",
      "Processed 2100/2803 rows (Source: medlinePlus.com)\n",
      "Processed 2200/3449 rows (Source: testing.com)\n",
      "Processed 1900/2906 rows (Dynamic Source)\n",
      "Processed 2200/2803 rows (Source: medlinePlus.com)\n",
      "Processed 2300/3449 rows (Source: testing.com)\n",
      "Processed 2000/2906 rows (Dynamic Source)\n",
      "Processed 2300/2803 rows (Source: medlinePlus.com)\n",
      "Processed 2400/3449 rows (Source: testing.com)\n",
      "Processed 2400/2803 rows (Source: medlinePlus.com)\n",
      "Processed 2100/2906 rows (Dynamic Source)\n",
      "Processed 2500/3449 rows (Source: testing.com)\n",
      "Processed 2500/2803 rows (Source: medlinePlus.com)\n",
      "Processed 2200/2906 rows (Dynamic Source)\n",
      "Processed 2600/3449 rows (Source: testing.com)\n",
      "Processed 2600/2803 rows (Source: medlinePlus.com)\n",
      "Processed 2700/3449 rows (Source: testing.com)\n",
      "Processed 2300/2906 rows (Dynamic Source)\n",
      "Processed 2700/2803 rows (Source: medlinePlus.com)\n",
      "Processed 2800/3449 rows (Source: testing.com)\n",
      "Processed 2400/2906 rows (Dynamic Source)\n",
      "Processed 2800/2803 rows (Source: medlinePlus.com)\n",
      "✅ Successfully processed medlineplus\n",
      "Processed 2900/3449 rows (Source: testing.com)\n",
      "Processed 2500/2906 rows (Dynamic Source)\n",
      "Processed 3000/3449 rows (Source: testing.com)\n",
      "Processed 2600/2906 rows (Dynamic Source)\n",
      "Processed 3100/3449 rows (Source: testing.com)\n",
      "Processed 2700/2906 rows (Dynamic Source)\n",
      "Processed 3200/3449 rows (Source: testing.com)\n",
      "Processed 3300/3449 rows (Source: testing.com)\n",
      "Processed 2800/2906 rows (Dynamic Source)\n",
      "Processed 3400/3449 rows (Source: testing.com)\n",
      "Processed 2900/2906 rows (Dynamic Source)\n",
      "✅ Successfully processed testing\n",
      "✅ Successfully processed files\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load CSV files\n",
    "df1 = pd.read_csv(\"../DATA/Chunked_data/medlineplus_chunks.csv\")\n",
    "df2 = pd.read_csv(\"../DATA/Chunked_data/testing_chunks.csv\")\n",
    "df3 = pd.read_csv(\"../DATA/Chunked_data/files_chunks.csv\")\n",
    "\n",
    "# Load the embedding model once (reduces overhead)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def embeddingCreator(text):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given text using SentenceTransformer.\n",
    "    \"\"\"\n",
    "    return embedding_model.encode(text).tolist()\n",
    "\n",
    "def Scrapper_With_Source(df, source):\n",
    "    \"\"\"\n",
    "    Extracts embeddings and metadata from a DataFrame when a fixed source is given.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for i, row in df.iterrows():\n",
    "        text = row[\"Description\"]\n",
    "        test_name = row[\"Test Name\"]\n",
    "        url = row[\"URL\"]\n",
    "\n",
    "        tokens = embeddingCreator(text)\n",
    "        data.append({\"Test Name\": test_name, \"tokens\": tokens, \"text\": text, \"Source\": source, \"URL\": url})\n",
    "        \n",
    "        if i % 100 == 0:  # Print progress every 100 rows\n",
    "            print(f\"Processed {i}/{len(df)} rows (Source: {source})\")\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def Scrapper_With_Out_Source(df):\n",
    "    \"\"\"\n",
    "    Extracts embeddings and metadata from a DataFrame when the source column exists.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for i, row in df.iterrows():\n",
    "        text = row[\"Description\"]\n",
    "        test_name = row[\"Test Name\"]\n",
    "        url = row[\"URL\"]\n",
    "        source = row[\"Source\"]\n",
    "\n",
    "        tokens = embeddingCreator(text)\n",
    "        data.append({\"Test Name\": test_name, \"tokens\": tokens, \"text\": text, \"Source\": source, \"URL\": url})\n",
    "\n",
    "        if i % 100 == 0:  # Print progress every 100 rows\n",
    "            print(f\"Processed {i}/{len(df)} rows (Dynamic Source)\")\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def writeToFile(df_tokens, filename):\n",
    "    \"\"\"\n",
    "    Writes the DataFrame to a CSV file.\n",
    "    \"\"\"\n",
    "    df_tokens.to_csv(filename, index=False, encoding=\"utf-8\")\n",
    "\n",
    "def Create_Embedding_With_Source(df, source, path):\n",
    "    \"\"\"\n",
    "    Wrapper function to process a DataFrame and store embeddings when source is known.\n",
    "    \"\"\"\n",
    "    df[\"Description\"] = df[\"Description\"].fillna('')\n",
    "    df_tokens = Scrapper_With_Source(df, source)\n",
    "    writeToFile(df_tokens, path)\n",
    "\n",
    "def Create_Embedding_With_Out_Source(df, path):\n",
    "    \"\"\"\n",
    "    Wrapper function to process a DataFrame and store embeddings when source is in CSV.\n",
    "    \"\"\"\n",
    "    df[\"Description\"] = df[\"Description\"].fillna('')\n",
    "    df_tokens = Scrapper_With_Out_Source(df)\n",
    "    writeToFile(df_tokens, path)\n",
    "\n",
    "# Parallel execution of embedding creation\n",
    "if __name__ == \"__main__\":\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = {\n",
    "            executor.submit(Create_Embedding_With_Out_Source, df3, \"../DATA/Embedded_Files/files_tokens.csv\"): \"files\",\n",
    "            executor.submit(Create_Embedding_With_Source, df1, \"medlinePlus.com\", \"../DATA/Embedded_Files/medlinePlus_tokens.csv\"): \"medlineplus\",\n",
    "            executor.submit(Create_Embedding_With_Source, df2, \"testing.com\", \"../DATA/Embedded_Files/testing_com_tokens.csv\"): \"testing\",\n",
    "        }\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            task_name = futures[future]\n",
    "            try:\n",
    "                future.result()  # This will raise exceptions if any occur\n",
    "                print(f\"✅ Successfully processed {task_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing {task_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
